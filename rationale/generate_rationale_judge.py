import json
import argparse
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer


def generate_rationale(result_file, distill_et_file, output_file, data_key, model_name, temperature, max_tokens):
    # 加载数据
    with open(result_file, 'r') as f1:
        data1 = json.load(f1)
    with open(distill_et_file, 'r') as f2:
        data2 = json.load(f2)

    # 确保两个文件的长度一致
    if len(data1[data_key]) != len(data2[data_key]):
        raise ValueError(f"两个文件的 '{data_key}' 数据长度不一致，无法比较！")

    # 初始化 vllm 模型和 tokenizer
    llm = LLM(model=model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    sampling_params = SamplingParams(
        temperature=temperature, max_tokens=max_tokens, skip_special_tokens=False)

    # 构造 prompts 列表
    conversation_list = []
    first = 0
    updated_instructions = []
    updated_responses = []

    for item1, item2 in zip(data1[data_key], data2[data_key]):
        if item2["result"]["orig"]["is_correct"] == True:
            if item1["result"]["orig"]["prediction"] == item2["result"]["orig"]["prediction"]:
                continue
        elif item2["result"]["swap"]["is_correct"] == True:
            if item1["result"]["swap"]["prediction"] == item2["result"]["swap"]["prediction"]:
                continue
        if item2["result"]["orig"]["prediction"] == 1:
            chosen = "Output (a)"
            rejected = "Output (b)"
        elif item2["result"]["orig"]["prediction"] == 2:
            chosen = "Output (b)"
            rejected = "Output (a)"
        else:
            if item1["result"]["orig"]["prediction"] == 1:
                chosen = "Output (a)"
                rejected = "Output (b)"
            elif item1["result"]["orig"]["prediction"] == 2:
                chosen = "Output (b)"
                rejected = "Output (a)"
            else:
                continue

        instruction = item1["instruction"]
        response1 = item1["response1"]
        response2 = item1["response2"]

        if first == 0:
            if chosen == "Output (b)":
                chosen, rejected = rejected, chosen
                response1, response2 = response2, response1
            first = 1
        elif first == 1:
            if chosen == "Output (a)":
                chosen, rejected = rejected, chosen
                response1, response2 = response2, response1
            first = 0

        updated_instructions.append(item1["instruction"])
        updated_responses.append((response1, response2))

        # 处理原始模型的结果
        conversation = construct_prompt(
            instruction, response1, response2, chosen, rejected)
        conversation_list.append(conversation)

    prompt_token_ids = [tokenizer.apply_chat_template(
        conversation, add_generation_prompt=True) for conversation in conversation_list]

    # Batch generate responses
    outputs = llm.generate(
        prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)

    # Print the first conversation prompt and its tokenized form
    print(
        f"Sampled conversation:\n{tokenizer.decode(prompt_token_ids[0], skip_special_tokens=False)}", end='')
    # Print the first generated text and its tokenized form
    print(f"{outputs[0].outputs[0].text.strip()}")
    generated_texts = [output.outputs[0].text.strip() for output in outputs]

    # 构造新数据
    formatted_data = []
    for generated_text, instruction, (response1, response2) in zip(generated_texts, updated_instructions, updated_responses):
        # 构造符合格式的数据
        formatted_data.append({
            "system": "You are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to select the best output for the given instruction.",
            "instruction": f"""After giving a brief explanation, select the Output (a) or Output (b) that is better for the given instruction. The two outputs are generated by two different AI chatbots respectively.

Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. For example, the order in which the outputs were presented should NOT affect your judgment, as Output (a) and Output (b) are **equally likely** to be the better.

You should first provide a brief explanation of your evaluation, and then always end your response with either "Therefore, Output (a) is better." or "Therefore, Output (b) is better." verbatim.
Do NOT say both / neither are good.
Do NOT output any other words.

# Instruction:
{instruction}

# Output (a):
{response1}

# Output (b):
{response2}

# Decision (Give a brief explanation of your evaluation followed by either "Therefore, Output (a) is better." or "Therefore, Output (b) is better." verbatim. In your explanation, you should always use "Output (a)" or "Output (b)" to refer to the two outputs respectively.):""",
            "input": "",
            "output": generated_text
        })

    # 保存结果到文件
    with open(output_file, 'w') as f:
        json.dump(formatted_data, f, indent=4)

    print(f"生成的数据已保存到 {output_file}")
    print(f"最终保存的数据数量: {len(formatted_data)}")


def construct_prompt(instruction, response1, response2, chosen, rejected):
    system = """You are an evaluation expert. Your goal is to provide the rationale based on the given evaluation result."""

    user = f"""As an evaluation expert, given an instruction and its two possible outputs, compare the outputs. Below are the instruction and its candidate outputs:

# Instruction:
{instruction}

# Output (a):
{response1}

# Output (b):
{response2}


Given that {chosen} is better than {rejected}, please provide the rationale and end with "Therefore, {chosen} is better.":"""
    return [{"role": "system", "content": system}, {"role": "user", "content": user}]


if __name__ == "__main__":
    # 使用 argparse 解析命令行参数
    parser = argparse.ArgumentParser(
        description="Use vllm to generate rationales and format the data.")
    parser.add_argument("--result", type=str, required=True,
                        help="Path to the original model results JSON file.")
    parser.add_argument("--result_distill_et", type=str, required=True,
                        help="Path to the Distill ET results JSON file.")
    parser.add_argument("--output", type=str, required=True,
                        help="Path to the output file.")
    parser.add_argument("--data", type=str, default="arena",
                        help="Key for accessing data (default: arena).")
    parser.add_argument("--model", type=str, default="gpt-4",
                        help="Name of the vllm model to use (default: gpt-4).")
    parser.add_argument("--temperature", type=float, default=0.7,
                        help="Temperature for generation randomness.")
    parser.add_argument("--max_tokens", type=int, default=512,
                        help="Maximum number of tokens to generate.")

    # 解析命令行参数
    args = parser.parse_args()

    # 调用主函数
    generate_rationale(
        result_file=args.result,
        distill_et_file=args.result_distill_et,
        output_file=args.output,
        data_key=args.data,
        model_name=args.model,
        temperature=args.temperature,
        max_tokens=args.max_tokens
    )

